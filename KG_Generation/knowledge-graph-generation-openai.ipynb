{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7419363,"sourceType":"datasetVersion","datasetId":4316415},{"sourceId":7496844,"sourceType":"datasetVersion","datasetId":4365244}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain neo4j openai wikipedia tiktoken langchain_openai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-01T10:34:19.050221Z","iopub.execute_input":"2024-02-01T10:34:19.051039Z","iopub.status.idle":"2024-02-01T10:34:31.164921Z","shell.execute_reply.started":"2024-02-01T10:34:19.051001Z","shell.execute_reply":"2024-02-01T10:34:31.163035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.graphs import Neo4jGraph\n\nurl = \"neo4j+s://815e74eb.databases.neo4j.io\"\nusername =\"neo4j\"\npassword = \"VoQigCFun5R1oK5zvXderMxODDQDbbPHTjc3U5sOCis\"\ngraph = Neo4jGraph(\n    url=url,\n    username=username,\n    password=password\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:43:25.595297Z","iopub.execute_input":"2024-02-01T10:43:25.596107Z","iopub.status.idle":"2024-02-01T10:43:28.934069Z","shell.execute_reply.started":"2024-02-01T10:43:25.596070Z","shell.execute_reply":"2024-02-01T10:43:28.933055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_community.graphs.graph_document import (\n    Node as BaseNode,\n    Relationship as BaseRelationship,\n    GraphDocument,\n)\nfrom langchain.schema import Document\nfrom typing import List, Dict, Any, Optional\nfrom langchain.pydantic_v1 import Field, BaseModel\n\nclass Property(BaseModel):\n  \"\"\"A single property consisting of key and value\"\"\"\n  key: str = Field(..., description=\"key\")\n  value: str = Field(..., description=\"value\")\n\nclass Node(BaseNode):\n    properties: Optional[List[Property]] = Field(\n        None, description=\"List of node properties\")\n\nclass Relationship(BaseRelationship):\n    properties: Optional[List[Property]] = Field(\n        None, description=\"List of relationship properties\"\n    )\n\nclass KnowledgeGraph(BaseModel):\n    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n    nodes: List[Node] = Field(\n        ..., description=\"List of nodes in the knowledge graph\")\n    rels: List[Relationship] = Field(\n        ..., description=\"List of relationships in the knowledge graph\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:43:42.091040Z","iopub.execute_input":"2024-02-01T10:43:42.091815Z","iopub.status.idle":"2024-02-01T10:43:42.245522Z","shell.execute_reply.started":"2024-02-01T10:43:42.091777Z","shell.execute_reply":"2024-02-01T10:43:42.244766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_property_key(s: str) -> str:\n    words = s.split()\n    if not words:\n        return s\n    first_word = words[0].lower()\n    capitalized_words = [word.capitalize() for word in words[1:]]\n    return \"\".join([first_word] + capitalized_words)\n\ndef props_to_dict(props) -> dict:\n    \"\"\"Convert properties to a dictionary.\"\"\"\n    properties = {}\n    if not props:\n      return properties\n    for p in props:\n        properties[format_property_key(p.key)] = p.value\n    return properties\n\ndef map_to_base_node(node: Node) -> BaseNode:\n    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n    properties = props_to_dict(node.properties) if node.properties else {}\n    # Add name property for better Cypher statement generation\n    properties[\"name\"] = node.id.title()\n    return BaseNode(\n        id=node.id.title(), type=node.type.capitalize(), properties=properties\n    )\n\n\ndef map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n    source = map_to_base_node(rel.source)\n    target = map_to_base_node(rel.target)\n    properties = props_to_dict(rel.properties) if rel.properties else {}\n    return BaseRelationship(\n        source=source, target=target, type=rel.type, properties=properties\n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.494472Z","iopub.status.idle":"2024-02-01T10:34:34.494889Z","shell.execute_reply.started":"2024-02-01T10:34:34.494707Z","shell.execute_reply":"2024-02-01T10:34:34.494725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openai==1.6.1\n!pip show openai","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.496176Z","iopub.status.idle":"2024-02-01T10:34:34.496551Z","shell.execute_reply.started":"2024-02-01T10:34:34.496347Z","shell.execute_reply":"2024-02-01T10:34:34.496363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pydantic==1.10.13\n!pip show pydantic","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.497520Z","iopub.status.idle":"2024-02-01T10:34:34.497885Z","shell.execute_reply.started":"2024-02-01T10:34:34.497718Z","shell.execute_reply":"2024-02-01T10:34:34.497735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom langchain.chains.openai_functions import (\n    create_openai_fn_chain,\n    create_structured_output_chain,\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"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\"\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n\ndef get_extraction_chain(\n    allowed_nodes: Optional[List[str]] = None,\n    allowed_rels: Optional[List[str]] = None\n    ):\n    prompt = ChatPromptTemplate.from_messages(\n        [(\n          \"system\",\n          f\"\"\"# Knowledge Graph Instructions for GPT-4\n## 1. Overview\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n## 2. Labeling Nodes\n- **Consistency**: Ensure you use basic or elementary types for node labels.\n  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n## 3. Handling Numerical Data and Dates\n- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n- **Property Format**: Properties must be in a key-value format.\n- **Quotation Marks**: Never use escaped single or double quotes within property values.\n- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n## 4. Coreference Resolution\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\nalways use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n## 5. Strict Compliance\nAdhere to the rules strictly. Non-compliance will result in termination.\n          \"\"\"),\n            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n        ])\n    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.499464Z","iopub.status.idle":"2024-02-01T10:34:34.499819Z","shell.execute_reply.started":"2024-02-01T10:34:34.499654Z","shell.execute_reply":"2024-02-01T10:34:34.499670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_and_store_graph(\n    document: Document,\n    nodes:Optional[List[str]] = None,\n    rels:Optional[List[str]]=None) -> None:\n    # Extract graph data using OpenAI functions\n    extract_chain = get_extraction_chain(nodes, rels)\n    data = extract_chain.invoke(document.page_content)['function']\n    # Construct a graph document\n    graph_document = GraphDocument(\n      nodes = [map_to_base_node(node) for node in data.nodes],\n      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n      source = document\n    )\n    # Store information into a graph\n    graph.add_graph_documents([graph_document])","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.500874Z","iopub.status.idle":"2024-02-01T10:34:34.501252Z","shell.execute_reply.started":"2024-02-01T10:34:34.501050Z","shell.execute_reply":"2024-02-01T10:34:34.501066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\naila_directory = \"/kaggle/input/aila-dataset/AILA_2019_Dataset\"\n\nquery_document_relevance_pairings = pd.read_csv(aila_directory + '/relevance_judgments_priorcases.txt', delimiter = \" \", header = None)\nquery_document_relevance_pairings.columns = [\"Query_Name\", \"Q0\", \"Document_Name\" ,\"Relevance\"]\nquery_document_relevance_pairings = query_document_relevance_pairings.drop(columns=[\"Q0\"])\n# query_document_relevance_pairings.head()\n\nrelevant_docs = []\nfor i, row in query_document_relevance_pairings.iterrows():\n    if row['Relevance'] == 1:\n        relevant_docs.append(row['Document_Name'])\nrelevant_docs = list(set(relevant_docs))\nprint(sorted(relevant_docs))\nprint(len(relevant_docs))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.503760Z","iopub.status.idle":"2024-02-01T10:34:34.504075Z","shell.execute_reply.started":"2024-02-01T10:34:34.503920Z","shell.execute_reply":"2024-02-01T10:34:34.503935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nfrom langchain.text_splitter import TokenTextSplitter\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.505984Z","iopub.status.idle":"2024-02-01T10:34:34.506379Z","shell.execute_reply.started":"2024-02-01T10:34:34.506181Z","shell.execute_reply":"2024-02-01T10:34:34.506201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport csv","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.507886Z","iopub.status.idle":"2024-02-01T10:34:34.508229Z","shell.execute_reply.started":"2024-02-01T10:34:34.508060Z","shell.execute_reply":"2024-02-01T10:34:34.508077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allowed_nodes = [\n    \"Accused\",\n    \"Acts\",\n    \"Advisory Jurisdiction\",\n    \"Apex Court\",\n    \"Appeal\",\n    \"Appellant\",\n    \"Appellant Jurisdiction\",\n    \"Author\",\n    \"Bench\",\n    \"Case\",\n    \"Case Type\",\n    \"Chief Metropolitan Court\",\n    \"City Civil Courts\",\n    \"Civil\",\n    \"Civil Courts\",\n    \"Country\",\n    \"Court Decision\",\n    \"Court\",\n    \"Court Judgements\",\n    \"Courts\",\n    \"Courts of Smaller Causes\",\n    \"Criminal\",\n    \"Criminal Courts\",\n    \"District\",\n    \"District Court\",\n    \"District Courts\",\n    \"Division Bench\",\n    \"Document\",\n    \"Evidence\",\n    \"FIR\",\n    \"Government\",\n    \"Group\",\n    \"High Court\",\n    \"Individual\",\n    \"Investigator\",\n    \"Judge\",\n    \"Judgement\",\n    \"Judicial Magistrate Court (First Class)\",\n    \"Judicial Magistrate Court (Second Class)\",\n    \"Jurisdiction\",\n    \"Larger Bench\",\n    \"Legal Participants\",\n    \"Location\",\n    \"Metropolitan Court\",\n    \"Metropolitan Magistrate Courts\",\n    \"Munsif Court\",\n    \"Non-Legal Participants\",\n    \"Order\",\n    \"Organization\",\n    \"Original Jurisdiction\",\n    \"Others\",\n    \"Participants\",\n    \"Petition\",\n    \"Petitioner\",\n    \"Place\",\n    \"Plaintiff\",\n    \"Precedent Case\",\n    \"Principal Junior Civil Court\",\n    \"Respondent\",\n    \"Review Jurisdiction\",\n    \"Sessions Court\",\n    \"Single Judge\",\n    \"Solicitor\",\n    \"Special Bench\",\n    \"State\",\n    \"Sub Court\",\n    \"Taluka\",\n    \"Tribunal Bench\",\n    \"Tribunals\",\n    \"Witness\",\n    \"Writ Jurisdiction\"\n]\n\nallowed_relations = [\n   \"caseBelongsToType\",\n   \"documentType\",\n   \"hasActs\",\n   \"hasAdvisoryJurisdiction\",\n   \"hasAppellantJurisdiction\",\n   \"hasAuthor\",\n   \"hasBench\",\n   \"hasCourtDecision\",\n   \"hasCourts\",\n   \"hasEvidenceLocation\",\n   \"hasEvidences\",\n   \"hasIndividuals\",\n   \"hasLegalParticipants\",\n   \"hasLocation\",\n   \"hasNonLegalParticipants\",\n   \"hasOriginalJurisdiction\",\n   \"hasParticipantType\",\n   \"hasPrecedentCase\",\n   \"hasReviewJurisdiction\",\n   \"hasWritJurisdiction\",\n   \"isA\",\n]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.509410Z","iopub.status.idle":"2024-02-01T10:34:34.509763Z","shell.execute_reply.started":"2024-02-01T10:34:34.509592Z","shell.execute_reply":"2024-02-01T10:34:34.509609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dir = aila_directory\noutput_dir = \"/kaggle/working/\"\n\n# Delete the graph\ngraph.query(\"MATCH (n) DETACH DELETE n\")\n\nfor j in range(1, 151): \n    \n    # Specify the path to your dataset folder\n    filename1 = input_dir + \"/Object_casedocs/C\" + str(j) + \".txt\"\n\n    document_instances = []\n\n    document_instance = namedtuple('Document', ['page_content', 'metadata'])\n    with open(filename1, 'r', encoding='utf-8') as file:\n        document_instances.append(document_instance(page_content=file.read(), metadata={'filename': filename1}))\n\n    text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n\n    # Split the selected document into chunks\n    documents = text_splitter.split_documents(document_instances)\n    \n    for i, d in tqdm(enumerate(documents), total=len(documents)):\n        extract_and_store_graph(d, allowed_nodes, allowed_relations)\n    #     time.sleep(1)    \n    \n    graph_result = graph.query(\"\"\"\n    MATCH (a)-[r]->(b)\n    RETURN labels(a) AS source_labels, a, type(r) AS relationship_type, properties(r) AS relationship_properties, labels(b) AS target_labels, b\n    \"\"\")\n    \n    output_file = output_dir + \"KG_C\" + str(j) + \".csv\"\n\n    with open(output_file, 'w', newline='') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow([\"n_type\", \"n\", \"r_type\", \"r\", \"m_type\", \"m\"])\n        for record in graph_result:\n            csv_writer.writerow([record[\"source_labels\"], record[\"a\"], record[\"relationship_type\"], record[\"relationship_properties\"], record[\"target_labels\"], record[\"b\"]])\n    \n    # Delete the graph\n    graph.query(\"MATCH (n) DETACH DELETE n\")\n            \n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.510854Z","iopub.status.idle":"2024-02-01T10:34:34.511166Z","shell.execute_reply.started":"2024-02-01T10:34:34.511006Z","shell.execute_reply":"2024-02-01T10:34:34.511020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# for i, d in tqdm(enumerate(documents), total=len(documents)):\n#     extract_and_store_graph(d)\n# #     time.sleep(1)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.513028Z","iopub.status.idle":"2024-02-01T10:34:34.513341Z","shell.execute_reply.started":"2024-02-01T10:34:34.513185Z","shell.execute_reply":"2024-02-01T10:34:34.513199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import time\n# from langchain.chains.openai_functions import (\n#     create_openai_fn_chain,\n#     create_structured_output_chain,\n# )\n# from langchain_openai import ChatOpenAI\n# from langchain.prompts import ChatPromptTemplate\n\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n# llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n\n# # Define rate limits\n# tokens_per_minute_limit = 40000\n# requests_per_minute_limit = 3\n# requests_per_day_limit = 200\n\n# tokens_used = 0\n# requests_made_today = 0\n# last_minute_timestamp = None\n\n# def get_extraction_chain(\n#     allowed_nodes: Optional[List[str]] = None,\n#     allowed_rels: Optional[List[str]] = None\n#     ):\n#     prompt = ChatPromptTemplate.from_messages(\n#         [(\n#           \"system\",\n#           f\"\"\"# Knowledge Graph Instructions for GPT-4\n# ## 1. Overview\n# You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n# - **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n# - The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n# ## 2. Labeling Nodes\n# - **Consistency**: Ensure you use basic or elementary types for node labels.\n#   - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n# - **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n# {'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n# {'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n# ## 3. Handling Numerical Data and Dates\n# - Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n# - **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n# - **Property Format**: Properties must be in a key-value format.\n# - **Quotation Marks**: Never use escaped single or double quotes within property values.\n# - **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n# ## 4. Coreference Resolution\n# - **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n# If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n# always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n# Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n# ## 5. Strict Compliance\n# Adhere to the rules strictly. Non-compliance will result in termination.\n#           \"\"\"),\n#             (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n#             (\"human\", \"Tip: Make sure to answer in the correct format\"),\n#         ])\n#     return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n\n# def extract_and_store_graph(document, nodes=None, rels=None):\n#     global tokens_used, requests_made_today, last_minute_timestamp\n\n#     # Check tokens used in the last minute\n#     current_minute = int(time.time() / 60)\n#     if last_minute_timestamp != current_minute:\n#         tokens_used = 0\n#         last_minute_timestamp = current_minute\n\n#     if tokens_used >= tokens_per_minute_limit:\n#         print(\"Exceeded tokens per minute limit. Waiting...\")\n#         time.sleep(60 - (time.time() % 60))\n#         tokens_used = 0\n\n#     # Check requests made in the last minute\n#     if requests_made_today >= requests_per_day_limit:\n#         print(\"Exceeded requests per day limit. Exiting...\")\n#         return None\n\n#     # Make the API request\n#     extract_chain = get_extraction_chain(nodes, rels)\n#     data = extract_chain.invoke(document.page_content)['function']\n\n#     # Update usage statistics\n#     tokens_used += data.tokens_used\n#     requests_made_today += 1\n\n#     # Construct a graph document and store information into a graph\n#     graph_document = GraphDocument(\n#         nodes=[map_to_base_node(node) for node in data.nodes],\n#         relationships=[map_to_base_relationship(rel) for rel in data.rels],\n#         source=document\n#     )\n#     graph.add_graph_documents([graph_document])\n\n# from collections import namedtuple\n# from langchain.text_splitter import TokenTextSplitter\n\n# # Specify the path to your dataset folder\n# dataset_folder = \"/kaggle/input/aila-casedocs/Object_casedocs\"\n\n# # Read only one text file from the folder\n# selected_document = None\n# for filename in os.listdir(dataset_folder):\n#     if filename.endswith(\".txt\"):\n#         file_path = os.path.join(dataset_folder, filename)\n#         with open(file_path, 'r', encoding='utf-8') as file:\n#             selected_document = file.read()\n#             break  # Stop after reading the first text file\n\n# # Define a named tuple for your document\n# Document = namedtuple('Document', ['page_content', 'metadata'])\n\n# # Create an instance of the Document named tuple\n# document_instance = Document(page_content=selected_document, metadata={'filename': filename})\n\n# # Define chunking strategy\n# text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n\n# # Split the selected document into chunks\n# documents = text_splitter.split_documents([document_instance])\n\n\n\n# from tqdm import tqdm\n\n# for i, d in tqdm(enumerate(documents), total=len(documents)):\n#     extract_and_store_graph(d)\n#     time.sleep(1)\n\n# # Print usage statistics\n# print(f\"Tokens used: {tokens_used}\")\n# print(f\"Requests made today: {requests_made_today}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.514464Z","iopub.status.idle":"2024-02-01T10:34:34.514930Z","shell.execute_reply.started":"2024-02-01T10:34:34.514691Z","shell.execute_reply":"2024-02-01T10:34:34.514712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import csv\n\n# graph_result = graph.query(\"\"\"\n# MATCH (a)-[r]->(b)\n# RETURN labels(a) AS source_labels, a, type(r) AS relationship_type, properties(r) AS relationship_properties, labels(b) AS target_labels, b\n# \"\"\")\n\n# with open(\"/kaggle/working/query-1.csv\", 'w', newline='') as csvfile:\n#     csv_writer = csv.writer(csvfile)\n#     csv_writer.writerow([\"n_type\", \"n\", \"r_type\", \"r\", \"m_type\", \"m\"])\n#     for record in graph_result:\n#         csv_writer.writerow([record[\"source_labels\"], record[\"a\"], record[\"relationship_type\"], record[\"relationship_properties\"], record[\"target_labels\"], record[\"b\"]])","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.516640Z","iopub.status.idle":"2024-02-01T10:34:34.517081Z","shell.execute_reply.started":"2024-02-01T10:34:34.516849Z","shell.execute_reply":"2024-02-01T10:34:34.516871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Delete the graph\n# graph.query(\"MATCH (n) DETACH DELETE n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T10:34:34.518265Z","iopub.status.idle":"2024-02-01T10:34:34.518730Z","shell.execute_reply.started":"2024-02-01T10:34:34.518483Z","shell.execute_reply":"2024-02-01T10:34:34.518505Z"},"trusted":true},"execution_count":null,"outputs":[]}]}