{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7163407,"sourceType":"datasetVersion","datasetId":4137803}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain neo4j openai wikipedia tiktoken langchain_openai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-23T12:15:30.289732Z","iopub.execute_input":"2024-03-23T12:15:30.290068Z","iopub.status.idle":"2024-03-23T12:16:29.241559Z","shell.execute_reply.started":"2024-03-23T12:15:30.290033Z","shell.execute_reply":"2024-03-23T12:16:29.240415Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/f8/1b/697dec4ff03114b049b687d4fdbdcefdfff365868876ec58c1ab2cf75253/langchain-0.1.13-py3-none-any.whl.metadata\n  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\nCollecting neo4j\n  Downloading neo4j-5.18.0.tar.gz (198 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.0/198.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting openai\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/c5/e7/5254c1c37a475d68b9ec11397a2fa967a06ef5e58e41755857f35b26511b/openai-1.14.2-py3-none-any.whl.metadata\n  Downloading openai-1.14.2-py3-none-any.whl.metadata (19 kB)\nCollecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting tiktoken\n  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/16/05/5efbd91252ffb1301ea393d88ef736b33d41e75d4bcf0bd31d660050e400/tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting langchain_openai\n  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/ea/db/66818d1efb4bdc037396b4e0c47c423f58e6bb23155b099bae50d816db43/langchain_openai-0.1.1-py3-none-any.whl.metadata\n  Downloading langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nCollecting jsonpatch<2.0,>=1.33 (from langchain)\n  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-community<0.1,>=0.0.29 (from langchain)\n  Obtaining dependency information for langchain-community<0.1,>=0.0.29 from https://files.pythonhosted.org/packages/e1/80/4136757bf245f60e2f4e148adb340207ea330be92bc26727bca32fe6bc48/langchain_community-0.0.29-py3-none-any.whl.metadata\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\nCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n  Obtaining dependency information for langchain-core<0.2.0,>=0.1.33 from https://files.pythonhosted.org/packages/d9/09/2aca6c80eed501c2de0990bb7b8b353f4baf67ad1f4117f1539e67847868/langchain_core-0.1.33-py3-none-any.whl.metadata\n  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Obtaining dependency information for langchain-text-splitters<0.1,>=0.0.1 from https://files.pythonhosted.org/packages/9d/a1/aec824080111e9b4a4802b51b988032faa193828c865e11233d1b18e88fa/langchain_text_splitters-0.0.1-py3-none-any.whl.metadata\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/f7/1d/6b238672b68c46db68b3682d181b68176feebd573cd084f87eea01fc54f9/langsmith-0.1.31-py3-none-any.whl.metadata\n  Downloading langsmith-0.1.31-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j) (2023.3)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nCollecting typing-extensions<5,>=4.7 (from openai)\n  Obtaining dependency information for typing-extensions<5,>=4.7 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Obtaining dependency information for orjson<4.0.0,>=3.9.14 from https://files.pythonhosted.org/packages/3f/22/8b8eba5adfcb5be89c85d74fc8e08b0913ebca264cfa7cbb3ffb7d1840c6/orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.13-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.14.2-py3-none-any.whl (262 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.1.1-py3-none-any.whl (32 kB)\nDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: neo4j, wikipedia\n  Building wheel for neo4j (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for neo4j: filename=neo4j-5.18.0-py3-none-any.whl size=273863 sha256=d45aab2c06eb235a916fee86f7ae48bb64a8c502084f1e40124b07d95bcccdfe\n  Stored in directory: /root/.cache/pip/wheels/e7/e1/a0/dd7c19192f5383ff57d02a6c126cbfe4b7b2ae82f70c6994ce\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=da78b08885cdf4b8f1766905dd27206e0f8e421a958e9715039a302752091043\n  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\nSuccessfully built neo4j wikipedia\nInstalling collected packages: typing-extensions, packaging, orjson, neo4j, jsonpatch, httpcore, wikipedia, tiktoken, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain-community, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.5\n    Uninstalling orjson-3.9.5:\n      Successfully uninstalled orjson-3.9.5\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.10.0 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langchain_openai-0.1.1 langsmith-0.1.31 neo4j-5.18.0 openai-1.14.2 orjson-3.9.15 packaging-23.2 tiktoken-0.6.0 typing-extensions-4.10.0 wikipedia-1.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.graphs import Neo4jGraph\n\nurl = \"neo4j+s://bd76b7b4.databases.neo4j.io:7687\"\nusername =\"neo4j\"\npassword = \"RRe_aWYD-Zlx5-gt1zLR-dsFEeKNJkFizV5HMX8Hb9M\"\ngraph = Neo4jGraph(\n    url=url,\n    username=username,\n    password=password\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:16:29.244087Z","iopub.execute_input":"2024-03-23T12:16:29.244924Z","iopub.status.idle":"2024-03-23T12:16:33.451033Z","shell.execute_reply.started":"2024-03-23T12:16:29.244886Z","shell.execute_reply":"2024-03-23T12:16:33.450291Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from langchain_community.graphs.graph_document import (\n    Node as BaseNode,\n    Relationship as BaseRelationship,\n    GraphDocument,\n)\nfrom langchain.schema import Document\nfrom typing import List, Dict, Any, Optional\nfrom langchain.pydantic_v1 import Field, BaseModel\n\nclass Property(BaseModel):\n  \"\"\"A single property consisting of key and value\"\"\"\n  key: str = Field(..., description=\"key\")\n  value: str = Field(..., description=\"value\")\n\n# Added \nclass SentenceMapping(BaseModel):\n  \"\"\"A single mapping consisting of key and value from a sentence number id to its sentence\"\"\"\n  key: int = Field(..., description=\"Sentence Number\")\n  value: str = Field(..., description=\"Sentence\")\n    \n# Added \nclass RrOccurrence(BaseModel):\n  \"\"\"A single mapping consisting of key and value from a sentence number id to the Rhetorical Role the sentence belongs to\"\"\"\n  key: int = Field(..., description=\"Sentence Number\")\n  value: str = Field(..., description=\"Rhetorical Role\")\n    \nclass Node(BaseNode):\n    properties: Optional[List[Property]] = Field(\n        None, description=\"List of node properties\")\n    # Added\n    rrOccurrences: List[RrOccurrence] = Field(\n        ..., description=\"List of Mappings of Sentence Numbers and the Rhetorical Role of Sentence where Node Occurs in a relationship\"\n    )\n\nclass Relationship(BaseRelationship):\n    properties: Optional[List[Property]] = Field(\n        None, description=\"List of relationship properties\"\n    )\n    # Added\n    rrOccurrences: List[RrOccurrence] = Field(\n        ..., description=\"List of Mappings of Sentence Numbers and the Rhetorical Role of Sentence where Relationship Occurs between nodes\"\n    )\n\nclass KnowledgeGraph(BaseModel):\n    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n    nodes: List[Node] = Field(\n        ..., description=\"List of nodes in the knowledge graph\")\n    rels: List[Relationship] = Field(\n        ..., description=\"List of relationships in the knowledge graph\"\n    )\n    # Added\n    sentenceMappings: List[SentenceMapping] = Field(..., description=\"List of Sentence Mappings from numbers to sentences\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:16:33.452186Z","iopub.execute_input":"2024-03-23T12:16:33.452696Z","iopub.status.idle":"2024-03-23T12:16:33.676088Z","shell.execute_reply.started":"2024-03-23T12:16:33.452670Z","shell.execute_reply":"2024-03-23T12:16:33.675334Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def format_property_key(s: str) -> str:\n    words = s.split()\n    if not words:\n        return s\n    first_word = words[0].lower()\n    capitalized_words = [word.capitalize() for word in words[1:]]\n    return \"\".join([first_word] + capitalized_words)\n\ndef props_to_dict(props) -> dict:\n    \"\"\"Convert properties to a dictionary.\"\"\"\n    properties = {}\n    if not props:\n      return properties\n    for p in props:\n        properties[format_property_key(p.key)] = p.value\n    return properties\n\ndef map_to_base_node(node: Node) -> BaseNode:\n    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n    properties = props_to_dict(node.properties) if node.properties else {}\n    properties[\"name\"] = node.id.title()\n    # Added\n    properties[\"occurrences\"] = props_to_dict(node.rrOccurrences) if node.rrOccurrences else {}\n    return BaseNode(\n        id=node.id.title(), type=node.type.capitalize(), properties=properties\n    )\n\ndef map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n    source = map_to_base_node(rel.source)\n    target = map_to_base_node(rel.target)\n    properties = props_to_dict(rel.properties) if rel.properties else {}\n    # Added\n    properties[\"occurrences\"] = props_to_dict(node.rrOccurrences) if node.rrOccurrences else {}\n    return BaseRelationship(\n        source=source, target=target, type=rel.type, properties=properties\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:16:33.677147Z","iopub.execute_input":"2024-03-23T12:16:33.677432Z","iopub.status.idle":"2024-03-23T12:16:33.687499Z","shell.execute_reply.started":"2024-03-23T12:16:33.677407Z","shell.execute_reply":"2024-03-23T12:16:33.686407Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install openai==1.6.1\n!pip show openai","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:16:33.690709Z","iopub.execute_input":"2024-03-23T12:16:33.691046Z","iopub.status.idle":"2024-03-23T12:16:58.056363Z","shell.execute_reply.started":"2024-03-23T12:16:33.691017Z","shell.execute_reply":"2024-03-23T12:16:58.055231Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting openai==1.6.1\n  Obtaining dependency information for openai==1.6.1 from https://files.pythonhosted.org/packages/e7/44/5ece9adb8b5943273c845a1e3200168b396f556051b7d2745995abf41584/openai-1.6.1-py3-none-any.whl.metadata\n  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.10.12)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (4.10.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.6.1) (3.4)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.6.1) (1.1.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.6.1) (2023.11.17)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.6.1) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.6.1) (0.14.0)\nDownloading openai-1.6.1-py3-none-any.whl (225 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: openai\n  Attempting uninstall: openai\n    Found existing installation: openai 1.14.2\n    Uninstalling openai-1.14.2:\n      Successfully uninstalled openai-1.14.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-openai 0.1.1 requires openai<2.0.0,>=1.10.0, but you have openai 1.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed openai-1.6.1\nName: openai\nVersion: 1.6.1\nSummary: The official Python library for the openai API\nHome-page: \nAuthor: \nAuthor-email: OpenAI <support@openai.com>\nLicense: \nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\nRequired-by: langchain-openai\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pydantic==1.10.13\n!pip show pydantic","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:16:58.057888Z","iopub.execute_input":"2024-03-23T12:16:58.058216Z","iopub.status.idle":"2024-03-23T12:17:23.456643Z","shell.execute_reply.started":"2024-03-23T12:16:58.058188Z","shell.execute_reply":"2024-03-23T12:17:23.455565Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pydantic==1.10.13\n  Obtaining dependency information for pydantic==1.10.13 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.13) (4.10.0)\nDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-openai 0.1.1 requires openai<2.0.0,>=1.10.0, but you have openai 1.6.1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-1.10.13\nName: pydantic\nVersion: 1.10.13\nSummary: Data validation and settings management using python type hints\nHome-page: https://github.com/pydantic/pydantic\nAuthor: Samuel Colvin\nAuthor-email: s@muelcolvin.com\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: typing-extensions\nRequired-by: confection, fastapi, langchain, langchain-core, langsmith, openai, spacy, thinc, vaex-core, weasel, ydata-profiling\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom langchain.chains.openai_functions import (\n    create_openai_fn_chain,\n    create_structured_output_chain,\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"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\"\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n\ndef get_extraction_chain(\n    allowed_nodes: Optional[List[str]] = None,\n    allowed_rels: Optional[List[str]] = None,\n    # Added rhetorical_roles\n    rhetorical_roles: Optional[List[str]] = None\n    ):\n    prompt = ChatPromptTemplate.from_messages(\n        [(\n          \"system\",\n          # Added \"Handling RrOccurences\"\n          f\"\"\"# Knowledge Graph Instructions for GPT-4\n## 1. Overview\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n## 2. Labeling Nodes\n- **Consistency**: Ensure you use basic or elementary types for node labels.\n  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n## 3. Handling RrOccurrences\n- **Definition**: RrOccurences is a list of mappings from the sentence of occurrence, to the Rhetorical Role Label of that Sentence that has been provided.\n- **Rule**: Rhetorical Roles (RRs) are labels applied to each 'sentence' in the document, where a 'sentence' is text in a single line, ie without a newline. \n- **Label Order**: The labels are applied one after the other in the order of sentence appearance.\n{'- **Labels:**' + \", \".join(rhetorical_roles) if rhetorical_roles else \"\"}\n## 4. Handling Numerical Data and Dates\n- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n- **Property Format**: Properties must be in a key-value format.\n- **Quotation Marks**: Never use escaped single or double quotes within property values.\n- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n## 5. Coreference Resolution\n- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\nalways use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n## 6. Strict Compliance\nAdhere to the rules strictly. Non-compliance will result in termination.\n          \"\"\"),\n            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n        ])\n    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:23.458046Z","iopub.execute_input":"2024-03-23T12:17:23.458355Z","iopub.status.idle":"2024-03-23T12:17:24.004473Z","shell.execute_reply.started":"2024-03-23T12:17:23.458327Z","shell.execute_reply":"2024-03-23T12:17:24.003709Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def extract_and_store_graph(\n    document: Document,\n    nodes:Optional[List[str]] = None,\n    rels:Optional[List[str]]=None) -> None:\n    extract_chain = get_extraction_chain(nodes, rels)\n    data = extract_chain.invoke(document.page_content)['function']\n    \n    # Added\n    sentenceMappings = data.sentenceMappings\n    \n    graph_document = GraphDocument(\n      nodes = [map_to_base_node(node) for node in data.nodes],\n      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n      source = document\n    )\n    graph.add_graph_documents([graph_document])\n    \n    return sentenceMappings","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.005676Z","iopub.execute_input":"2024-03-23T12:17:24.005929Z","iopub.status.idle":"2024-03-23T12:17:24.012370Z","shell.execute_reply.started":"2024-03-23T12:17:24.005906Z","shell.execute_reply":"2024-03-23T12:17:24.011392Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# aila_directory = \"/kaggle/input/aila-dataset/AILA_2019_Dataset\"\n\n# query_document_relevance_pairings = pd.read_csv(aila_directory + '/relevance_judgments_priorcases.txt', delimiter = \" \", header = None)\n# query_document_relevance_pairings.columns = [\"Query_Name\", \"Q0\", \"Document_Name\" ,\"Relevance\"]\n# query_document_relevance_pairings = query_document_relevance_pairings.drop(columns=[\"Q0\"])\n# # query_document_relevance_pairings.head()\n\n# relevant_docs = []\n# for i, row in query_document_relevance_pairings.iterrows():\n#     if row['Relevance'] == 1:\n#         relevant_docs.append(row['Document_Name'])\n# relevant_docs = list(set(relevant_docs))\n# print(sorted(relevant_docs))\n# print(len(relevant_docs))","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.013876Z","iopub.execute_input":"2024-03-23T12:17:24.014179Z","iopub.status.idle":"2024-03-23T12:17:24.028364Z","shell.execute_reply.started":"2024-03-23T12:17:24.014156Z","shell.execute_reply":"2024-03-23T12:17:24.027396Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nfrom langchain.text_splitter import TokenTextSplitter\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.029392Z","iopub.execute_input":"2024-03-23T12:17:24.029651Z","iopub.status.idle":"2024-03-23T12:17:24.044070Z","shell.execute_reply.started":"2024-03-23T12:17:24.029629Z","shell.execute_reply":"2024-03-23T12:17:24.043339Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport csv","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.045123Z","iopub.execute_input":"2024-03-23T12:17:24.045377Z","iopub.status.idle":"2024-03-23T12:17:24.057734Z","shell.execute_reply.started":"2024-03-23T12:17:24.045354Z","shell.execute_reply":"2024-03-23T12:17:24.056921Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"allowed_nodes = [\n    \"Accused\",\n    \"Acts\",\n    \"Advisory Jurisdiction\",\n    \"Apex Court\",\n    \"Appeal\",\n    \"Appellant\",\n    \"Appellant Jurisdiction\",\n    \"Author\",\n    \"Bench\",\n    \"Case\",\n    \"Case Type\",\n    \"Chief Metropolitan Court\",\n    \"City Civil Courts\",\n    \"Civil\",\n    \"Civil Courts\",\n    \"Country\",\n    \"Court Decision\",\n    \"Court\",\n    \"Court Judgements\",\n    \"Courts\",\n    \"Courts of Smaller Causes\",\n    \"Criminal\",\n    \"Criminal Courts\",\n    \"District\",\n    \"District Court\",\n    \"District Courts\",\n    \"Division Bench\",\n    \"Document\",\n    \"Evidence\",\n    \"FIR\",\n    \"Government\",\n    \"Group\",\n    \"High Court\",\n    \"Individual\",\n    \"Investigator\",\n    \"Judge\",\n    \"Judgement\",\n    \"Judicial Magistrate Court (First Class)\",\n    \"Judicial Magistrate Court (Second Class)\",\n    \"Jurisdiction\",\n    \"Larger Bench\",\n    \"Legal Participants\",\n    \"Location\",\n    \"Metropolitan Court\",\n    \"Metropolitan Magistrate Courts\",\n    \"Munsif Court\",\n    \"Non-Legal Participants\",\n    \"Order\",\n    \"Organization\",\n    \"Original Jurisdiction\",\n    \"Others\",\n    \"Participants\",\n    \"Petition\",\n    \"Petitioner\",\n    \"Place\",\n    \"Plaintiff\",\n    \"Precedent Case\",\n    \"Principal Junior Civil Court\",\n    \"Respondent\",\n    \"Review Jurisdiction\",\n    \"Sessions Court\",\n    \"Single Judge\",\n    \"Solicitor\",\n    \"Special Bench\",\n    \"State\",\n    \"Sub Court\",\n    \"Taluka\",\n    \"Tribunal Bench\",\n    \"Tribunals\",\n    \"Witness\",\n    \"Writ Jurisdiction\"\n]\n\nallowed_relations = [\n   \"caseBelongsToType\",\n   \"documentType\",\n   \"hasActs\",\n   \"hasAdvisoryJurisdiction\",\n   \"hasAppellantJurisdiction\",\n   \"hasAuthor\",\n   \"hasBench\",\n   \"hasCourtDecision\",\n   \"hasCourts\",\n   \"hasEvidenceLocation\",\n   \"hasEvidences\",\n   \"hasIndividuals\",\n   \"hasLegalParticipants\",\n   \"hasLocation\",\n   \"hasNonLegalParticipants\",\n   \"hasOriginalJurisdiction\",\n   \"hasParticipantType\",\n   \"hasPrecedentCase\",\n   \"hasReviewJurisdiction\",\n   \"hasWritJurisdiction\",\n   \"isA\",\n]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.059062Z","iopub.execute_input":"2024-03-23T12:17:24.059374Z","iopub.status.idle":"2024-03-23T12:17:24.068670Z","shell.execute_reply.started":"2024-03-23T12:17:24.059350Z","shell.execute_reply":"2024-03-23T12:17:24.067782Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"directory = \"/kaggle/input/paper-data/documents\"","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.069740Z","iopub.execute_input":"2024-03-23T12:17:24.070031Z","iopub.status.idle":"2024-03-23T12:17:24.081402Z","shell.execute_reply.started":"2024-03-23T12:17:24.070002Z","shell.execute_reply":"2024-03-23T12:17:24.080596Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_dir = directory\noutput_dir = \"/kaggle/working/\"\n\ndocument_sentence_mappings = []\n\n# Delete the graph\ngraph.query(\"MATCH (n) DETACH DELETE n\")\n\nfor j in range(1): \n    \n    # Specify the path to your dataset folder\n    # filename1 = input_dir + \"/Object_casedocs/C\" + str(j+1) + \".txt\"\n    filename1 = input_dir + \"/c\" + str(j+1) + \".txt\"\n\n    document_instances = []\n\n    document_instance = namedtuple('Document', ['page_content', 'metadata'])\n    with open(filename1, 'r', encoding='utf-8') as file:\n        document_instances.append(document_instance(page_content=file.read(), metadata={'filename': filename1}))\n\n    text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=24)\n\n    # Split the selected document into chunks\n    documents = text_splitter.split_documents(document_instances)\n    \n    # Added c0.txt -> RR\n    document_rhetorical_roles = [3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 5, 5, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 12, 4, 4]\n    \n    for i, d in tqdm(enumerate(documents), total=len(documents)):\n        try:\n            # Added document_rhetorical_roles\n            document_sentence_mappings.append(extract_and_store_graph(d, allowed_nodes, allowed_relations, document_rhetorical_roles))\n        except Exception as e:\n            print(f\"Error processing document {j}, {i}th section\")\n            print(e)\n            continue\n    \n    graph_result = graph.query(\"\"\"\n    MATCH (a)-[r]->(b)\n    RETURN labels(a) AS source_labels, a, type(r) AS relationship_type, properties(r) AS relationship_properties, labels(b) AS target_labels, b\n    \"\"\")\n    \n    output_file = output_dir + \"RR_Detailed_KG_C\" + str(j) + \".csv\"\n\n    with open(output_file, 'w', newline='') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow([\"n_type\", \"n\", \"r_type\", \"r\", \"m_type\", \"m\"])\n        for record in graph_result:\n            csv_writer.writerow([record[\"source_labels\"], record[\"a\"], record[\"relationship_type\"], record[\"relationship_properties\"], record[\"target_labels\"], record[\"b\"]])\n    \n    # Delete the graph\n    graph.query(\"MATCH (n) DETACH DELETE n\")\n            \n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:24.084496Z","iopub.execute_input":"2024-03-23T12:17:24.084751Z","iopub.status.idle":"2024-03-23T12:17:28.520091Z","shell.execute_reply.started":"2024-03-23T12:17:24.084729Z","shell.execute_reply":"2024-03-23T12:17:28.519274Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 24480.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Error processing document 0, 0th section\nextract_and_store_graph() takes from 1 to 3 positional arguments but 4 were given\nError processing document 0, 1th section\nextract_and_store_graph() takes from 1 to 3 positional arguments but 4 were given\nError processing document 0, 2th section\nextract_and_store_graph() takes from 1 to 3 positional arguments but 4 were given\n","output_type":"stream"}]},{"cell_type":"code","source":"print(document_sentence_mappings)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T12:17:28.521223Z","iopub.execute_input":"2024-03-23T12:17:28.521581Z","iopub.status.idle":"2024-03-23T12:17:28.526719Z","shell.execute_reply.started":"2024-03-23T12:17:28.521549Z","shell.execute_reply":"2024-03-23T12:17:28.525988Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}]}]}